<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GLARE Blog Post - Substack Ready</title>
    <style>
        body {
            font-family: 'Charter', 'Georgia', serif;
            max-width: 680px;
            margin: 40px auto;
            padding: 0 20px;
            line-height: 1.7;
            color: #1a1a1a;
            font-size: 18px;
        }
        h1 { font-size: 2em; line-height: 1.2; margin-top: 1.5em; }
        h2 { font-size: 1.5em; margin-top: 2em; }
        h3 { font-size: 1.2em; margin-top: 1.5em; }
        blockquote {
            border-left: 3px solid #ccc;
            margin: 1.5em 0;
            padding: 0.5em 1.5em;
            color: #555;
            font-style: italic;
        }
        .equation-note {
            background: #f0f4ff;
            border: 1px dashed #aac;
            padding: 10px 15px;
            margin: 1em 0;
            border-radius: 4px;
            font-family: monospace;
            font-size: 0.85em;
            color: #336;
        }
        .equation-note::before {
            content: "EQUATION (use Substack's equation button): ";
            font-weight: bold;
            color: #558;
        }
        hr {
            border: none;
            border-top: 1px solid #ddd;
            margin: 2.5em 0;
        }
        .subtitle {
            font-size: 1.1em;
            color: #555;
            font-style: italic;
            margin-top: -0.5em;
        }
        .results-block {
            background: #f9f9f9;
            border-left: 4px solid #2a9d8f;
            padding: 15px 20px;
            margin: 1.5em 0;
        }
        .results-block strong { color: #264653; }
        code {
            background: #f4f4f4;
            padding: 2px 5px;
            border-radius: 3px;
            font-size: 0.9em;
        }

        /* Query Example Styles */
        .query-box {
            background: #f8f9fa;
            border-left: 4px solid #3273dc;
            padding: 15px 20px;
            margin: 1.5em 0;
            border-radius: 0 6px 6px 0;
        }
        .query-box .q {
            font-weight: 600;
            color: #3273dc;
            margin-bottom: 5px;
        }
        .query-box .a {
            color: #2d6a2e;
            font-style: italic;
        }
        .query-box .sql {
            font-family: monospace;
            font-size: 0.82em;
            color: #555;
            background: #eef;
            padding: 8px 12px;
            border-radius: 4px;
            margin-top: 8px;
            overflow-x: auto;
        }

        /* Results Charts Styles */
        .results-chart-container {
            margin: 25px 0;
            padding: 20px;
            background: #fafafa;
            border-radius: 10px;
        }
        .chart {
            display: flex;
            flex-direction: column;
            gap: 6px;
        }
        .chart-row {
            display: flex;
            align-items: center;
            gap: 10px;
        }
        .chart-label {
            width: 140px;
            font-size: 0.8em;
            font-weight: 600;
            color: #444;
            text-align: right;
            flex-shrink: 0;
        }
        .chart-bar-container {
            flex: 1;
            height: 24px;
            background: #eee;
            border-radius: 4px;
            position: relative;
            overflow: hidden;
        }
        .chart-bar {
            height: 100%;
            border-radius: 4px;
            display: flex;
            align-items: center;
            justify-content: flex-end;
            padding-right: 8px;
            font-size: 0.7em;
            font-weight: 700;
            color: white;
        }
        .chart-bar.small-text {
            justify-content: flex-start;
            padding-left: 8px;
            color: #333;
        }
        .difficulty-section {
            margin-bottom: 15px;
        }
        .difficulty-header {
            font-weight: 700;
            font-size: 0.85em;
            color: #555;
            margin: 12px 0 6px 0;
            padding: 3px 8px;
            background: #f0f0f0;
            border-radius: 4px;
            display: inline-block;
        }
        .chart-title {
            text-align: center;
            margin-bottom: 15px;
        }
        .chart-title h4 {
            font-size: 1em;
            color: #333;
            margin: 0;
            font-style: normal;
        }
        .chart-title p {
            font-size: 0.75em;
            color: #888;
            margin: 3px 0 0;
        }
        .chart-caption {
            font-size: 0.75em;
            color: #777;
            margin-top: 12px;
            text-align: center;
            font-style: italic;
        }
        .chart-divider {
            border-top: 1px solid #ddd;
            margin: 6px 0 6px 140px;
        }
        .image-container {
            margin: 2em 0;
            text-align: center;
        }
        .image-container img {
            max-width: 100%;
            border-radius: 8px;
            border: 1px solid #e9ecef;
        }
        .image-caption {
            font-size: 0.85em;
            color: #666;
            margin-top: 8px;
            font-style: italic;
        }

        /* Insight box */
        .insight-box {
            background: #fffbea;
            border: 1px solid #f0d060;
            padding: 15px 20px;
            margin: 1.5em 0;
            border-radius: 6px;
        }
        .insight-box::before {
            content: "KEY INSIGHT: ";
            font-weight: bold;
            color: #b8860b;
        }
    </style>
</head>
<body>

<h1>Stop Explaining Models. Start Letting Users Ask Questions.</h1>

<p class="subtitle">How treating global explanations as databases&mdash;and LLMs as semantic parsers, not generators&mdash;creates an XAI interface that achieves 95% accuracy, transfers to new datasets at 90%, and sidesteps the hallucination problem entirely.</p>

<hr>

<h2>The Problem Nobody Talks About in XAI</h2>

<p>We've gotten remarkably good at <em>generating</em> explanations for deep learning models. Grad-CAM, SHAP, LIME, concept bottleneck models, prototype networks&mdash;the zoo of explanation methods grows every year. But here's the uncomfortable truth:</p>

<p><strong>Most explanations go unused.</strong></p>

<p>Not because they're wrong. Not because practitioners don't care about interpretability. But because the format is wrong. We hand users a static artifact&mdash;a saliency map, a set of rules, a list of prototypes&mdash;and say "here, understand your model." This is the equivalent of handing someone an encyclopedia when they asked a question.</p>

<p>Consider what happens when you compute a global explanation for a scene classifier trained on ADE20K. You might get hundreds of logical rules per class, expressed as disjunctive normal form (DNF) formulas:</p>

<div class="results-block">
    <strong>bedroom</strong> &larr; (bed &and; wall) &or; (bed &and; curtain &and; lamp) &or; (bed &and; pillow &and; nightstand) &or; ...<br>
    <strong>kitchen</strong> &larr; (stove &and; cabinet) &or; (oven &and; counter &and; sink) &or; (refrigerator &and; floor &and; cabinet) &or; ...<br>
    <em>... hundreds more rules per class, across 35 scene categories</em>
</div>

<p>No human is going to read all of this. But every human who encounters this model will have <em>specific questions</em>: Does the model use background features for classification? What distinguishes kitchen from dining room? How often does "bed" appear in bedroom explanations?</p>

<p>The fundamental mismatch is that <strong>explanations are generated as monologues, but understanding happens through dialogue</strong>.</p>

<hr>

<h2>The Idea: Explanations as Databases</h2>

<p>Our paper, <em>GLARE: A Natural Language Interface for Querying Global Explanations</em>, starts from a simple observation:</p>

<blockquote>Global explanations have relational structure. They describe relationships between objects, classes, images, and decision boundaries. This is exactly the kind of data that databases are designed to store and query.</blockquote>

<p>So instead of presenting explanations as a document to be read, we ingest them into a relational database. Each local explanation (a Minimal Sufficient Explanation, or MSX) becomes a row in a table. Objects, classes, confidence scores, and image IDs become columns. Suddenly, questions that would require a PhD student hours of manual analysis become SQL queries that execute in milliseconds.</p>

<p>The second observation is about LLMs. Everyone is using LLMs to <em>generate</em> explanations. We think this is the wrong job for them. LLMs hallucinate. When you ask an LLM to explain a model's behavior, it produces plausible-sounding text that may have nothing to do with how the model actually works.</p>

<div class="insight-box">
Use LLMs as <em>semantic parsers</em>, not generators. The LLM translates natural language questions into SQL. The answers come from the actual explanation data. The LLM never touches the content of the explanation&mdash;it only handles the <em>form</em> of the query.
</div>

<p>This gives you the best of both worlds: the flexibility of natural language input with the formal correctness of database queries. No hallucination possible, because every answer is computed deterministically from ground-truth data.</p>

<hr>

<h2>How It Works: A Walkthrough</h2>

<p>Let me trace through a concrete example to make this tangible.</p>

<p>A user asks:</p>

<div class="query-box">
    <div class="q">"What percentage of bedroom images contain both bed and wall?"</div>
</div>

<p>Here's what happens inside GLARE:</p>

<p><strong>Step 1: Query parsing.</strong> The fine-tuned LLM (Gemma 2-9B) receives the question along with a system prompt containing the database schema and allowed entity names. It identifies this as a "boolean AND percentage" query template and extracts the parameters: class = <code>bedroom</code>, objects = <code>bed</code>, <code>wall</code>.</p>

<p><strong>Step 2: SQL generation.</strong> The LLM outputs SQL between special fence markers:</p>

<div class="query-box">
    <div class="sql">SQL_START<br>
SELECT ROUND(COUNT(DISTINCT CASE WHEN io1.object_name = 'bed' AND io2.object_name = 'wall' THEN i.image_id END) * 100.0 / COUNT(DISTINCT i.image_id), 2) AS percentage FROM images i JOIN image_objects io1 ON i.image_id = io1.image_id JOIN image_objects io2 ON i.image_id = io2.image_id WHERE i.class_name = 'bedroom'<br>
SQL_END</div>
</div>

<p><strong>Step 3: Validation and execution.</strong> The SQL is parsed for syntactic correctness, checked for safety (no writes, no destructive operations), and executed against the explanation database.</p>

<p><strong>Step 4: Response generation.</strong> The result is formatted as: <em>"73.2% of bedroom images contain both bed and wall."</em> alongside supporting evidence images with highlighted objects.</p>

<p>The critical property: <strong>the LLM never sees the explanation data</strong>. It only translates the question into a formal query. The data remains the single source of truth.</p>

<hr>

<h2>The Training Pipeline: How You Teach an LLM SQL Without Any Manual Annotation</h2>

<p>One of the biggest challenges was generating training data. We needed thousands of (question, SQL) pairs, but manually writing them would be tedious and wouldn't scale to new datasets.</p>

<p>Our solution: <strong>fully synthetic training data</strong>.</p>

<p>We define 24 query templates organized into three tiers:</p>
<ul>
    <li><strong>Core queries</strong>: Object frequency, boolean combinations, top-k ranking, co-occurrence</li>
    <li><strong>Extended queries</strong>: N-way combinations (self-joins), cross-class comparison, set operations, conditional co-occurrence</li>
    <li><strong>Contrastive queries</strong>: Absence analysis, threshold filtering, distinguishing features, counterfactual reasoning</li>
</ul>

<p>Each template is a function that takes random parameters (class names, object names, thresholds) and outputs both a natural language question and the corresponding SQL. We generate 50,000 training examples this way. No human annotation. No expensive labeling.</p>

<p>But there's a subtlety. If you fine-tune on these examples naively, the model might memorize associations between specific entity names and SQL patterns. It would learn "when you see 'bed', put 'bed' in the WHERE clause" rather than "when you see an object name, put it in the WHERE clause."</p>

<p>Our solution is <strong>fence-based loss masking</strong>. During fine-tuning, we wrap the target SQL in special markers (<code>SQL_START</code> / <code>SQL_END</code>) and compute the training loss <em>only on the SQL tokens</em>. The model never receives gradient signal for the prompt tokens (which contain entity names and natural language). This forces it to learn the <em>relational algebra</em> of querying&mdash;the structural mapping from question patterns to SQL patterns&mdash;rather than dataset-specific vocabulary associations.</p>

<p>The proof is in the transfer results: a model trained entirely on ADE20K (with objects like "wall", "bed", "stove") transfers to Pascal VOC (with objects like "lear", "torso", "rhand") at <strong>90.6% accuracy</strong>. It has never seen these entity names during training. It learned the structure, not the vocabulary.</p>

<hr>

<h2>Results: What Actually Works (and What Doesn't)</h2>

<h3>The Headline Numbers</h3>

<p>On 500 held-out test queries from the ADE20K domain:</p>

<div class="results-chart-container">
    <div class="chart-title">
        <h4>In-Distribution Accuracy (ADE20K Fresh Test)</h4>
        <p>Fine-tuned models vs. base models on 500 held-out examples</p>
    </div>

    <div class="chart">
        <div class="chart-row">
            <div class="chart-label">Gemma 2 (9B) FT</div>
            <div class="chart-bar-container"><div class="chart-bar" style="width: 95.2%; background: #00A3A1;">95.2%</div></div>
        </div>
        <div class="chart-row">
            <div class="chart-label">Gemma 2 (27B) FT</div>
            <div class="chart-bar-container"><div class="chart-bar" style="width: 95.2%; background: #2ab8b6;">95.2%</div></div>
        </div>
        <div class="chart-row">
            <div class="chart-label">Gemma 2 (2B) FT</div>
            <div class="chart-bar-container"><div class="chart-bar" style="width: 95.4%; background: #4dc9c7;">95.4%</div></div>
        </div>
        <div class="chart-row">
            <div class="chart-label">Qwen 2.5 (14B) FT</div>
            <div class="chart-bar-container"><div class="chart-bar" style="width: 95.4%; background: #7ab8b6;">95.4%</div></div>
        </div>
        <div class="chart-row">
            <div class="chart-label">Qwen 2.5 (7B) FT</div>
            <div class="chart-bar-container"><div class="chart-bar" style="width: 93%; background: #9ec5c4;">93.0%</div></div>
        </div>
        <div class="chart-divider" style="margin-left:0"></div>
        <div class="chart-row">
            <div class="chart-label" style="color:#c0392b;">Gemma 2 (9B) Base</div>
            <div class="chart-bar-container"><div class="chart-bar small-text" style="width: 2%; background: #e74c3c;">0.2%</div></div>
        </div>
        <div class="chart-row">
            <div class="chart-label" style="color:#c0392b;">Qwen 2.5 (7B) Base</div>
            <div class="chart-bar-container"><div class="chart-bar small-text" style="width: 4%; background: #c0392b;">3.2%</div></div>
        </div>
    </div>
    <p class="chart-caption">Fine-tuned models converge at ~95% accuracy across model families and scales. Base models score near 0%.</p>
</div>

<p>Several things stand out:</p>

<ol>
    <li><strong>Performance saturates early.</strong> Gemma 2B, 9B, and 27B all hit ~95%. This means GLARE's query mapping is learnable even by small models. You don't need GPT-4-scale resources.</li>
    <li><strong>Base models achieve near zero.</strong> Even Gemma 2-27B, which presumably has strong SQL knowledge from pre-training, scores 0.0% on result-match accuracy. Our task is not "general SQL"&mdash;it requires learning the specific schema, template structures, and fence format. The synthetic training pipeline is essential.</li>
    <li><strong>There's a minimum capacity threshold.</strong> Qwen 2.5 at 0.5B achieves only 4.4%. Below ~2B parameters, models can't reliably learn the mapping. Above 2B, performance plateaus.</li>
</ol>

<h3>Per-Query-Type Breakdown: Where It's Perfect and Where It Struggles</h3>

<p>The model achieves <strong>100% accuracy on 18 of 25 query types</strong>. These include percentage calculations, top-k/bottom-k ranking, co-occurrence joins, set difference/intersection, conditional existence checks, and threshold filtering. These represent the most common user questions.</p>

<p>The two weakest types:</p>
<ul>
    <li><strong>N-way combinations</strong> (49%): Queries like "rarest 4-object pairs in bedroom?" require multi-way self-joins with ordering constraints. The model occasionally generates the wrong number of joins or misses the deduplication constraint.</li>
    <li><strong>Exact count queries</strong> (43%): "Images with exactly 3 different objects" requires a HAVING clause with exact equality&mdash;a pattern that's inherently fragile because small SQL errors (> vs. =) completely change the result.</li>
</ul>

<p>These failure modes are informative: the model struggles precisely where SQL requires exact structural precision in rarely-seen patterns. The fix is straightforward&mdash;increase the sampling frequency for these templates during synthetic data generation.</p>

<h3>Robustness: How Badly Can Users Type?</h3>

<p>Real users don't type clean, grammatically perfect questions. We tested 7 perturbation types:</p>

<div class="results-chart-container">
    <div class="chart-title">
        <h4>Robustness to Input Perturbations</h4>
        <p>% of baseline accuracy preserved under each perturbation type</p>
    </div>

    <div class="chart">
        <div class="chart-row">
            <div class="chart-label">Synonym substitution</div>
            <div class="chart-bar-container"><div class="chart-bar" style="width: 100%; background: #27ae60;">100%</div></div>
        </div>
        <div class="chart-row">
            <div class="chart-label">Verbose padding</div>
            <div class="chart-bar-container"><div class="chart-bar" style="width: 97%; background: #2ecc71;">97%</div></div>
        </div>
        <div class="chart-row">
            <div class="chart-label">Spelling errors</div>
            <div class="chart-bar-container"><div class="chart-bar" style="width: 89%; background: #f39c12;">89%</div></div>
        </div>
        <div class="chart-row">
            <div class="chart-label">Telegraphic</div>
            <div class="chart-bar-container"><div class="chart-bar" style="width: 84%; background: #e67e22;">84%</div></div>
        </div>
        <div class="chart-row">
            <div class="chart-label">Word swap</div>
            <div class="chart-bar-container"><div class="chart-bar" style="width: 82%; background: #e67e22;">82%</div></div>
        </div>
        <div class="chart-row">
            <div class="chart-label">Grammar corruption</div>
            <div class="chart-bar-container"><div class="chart-bar" style="width: 76%; background: #e74c3c;">76%</div></div>
        </div>
        <div class="chart-row">
            <div class="chart-label">Word drop</div>
            <div class="chart-bar-container"><div class="chart-bar" style="width: 48%; background: #c0392b;">48%</div></div>
        </div>
    </div>
    <p class="chart-caption">The model handles surface-level variation well. Word drop destroys information (removing entity names), so low robustness there is expected and correct.</p>
</div>

<p>The key finding: <strong>the learned SQL mapping is largely invariant to lexical and syntactic surface variation</strong>. Synonym substitution (100%) and verbose padding (97%) barely affect it. Spelling errors (89%) are handled gracefully because the model has learned to map approximate tokens to correct entity names.</p>

<p>Word drop (48%) is the outlier, but this makes sense: if you remove the word "bedroom" from "what objects appear in bedroom?", the model literally cannot know which class you mean. Low robustness to information-destroying perturbations is not a bug&mdash;it's a correct property of a well-calibrated system.</p>

<h3>The Transfer Result That Surprised Us</h3>

<p>We trained on ADE20K (150 objects, 20 scene classes) and tested on Pascal VOC (166 objects, completely different vocabulary, different task taxonomy). No retraining. No fine-tuning on Pascal VOC data. Just swap the entity name list in the prompt.</p>

<div class="results-chart-container">
    <div class="chart-title">
        <h4>Zero-Shot Cross-Dataset Transfer: ADE20K &rarr; Pascal VOC</h4>
        <p>Models trained on ADE20K, tested on Pascal VOC with no retraining</p>
    </div>

    <div class="chart">
        <div class="chart-row">
            <div class="chart-label">Gemma 2 (27B)</div>
            <div class="chart-bar-container"><div class="chart-bar" style="width: 90.6%; background: #2980b9;">90.6%</div></div>
        </div>
        <div class="chart-row">
            <div class="chart-label">Gemma 2 (2B)</div>
            <div class="chart-bar-container"><div class="chart-bar" style="width: 90%; background: #3498db;">90.0%</div></div>
        </div>
        <div class="chart-row">
            <div class="chart-label">Qwen 2.5 (14B)</div>
            <div class="chart-bar-container"><div class="chart-bar" style="width: 90%; background: #5dade2;">90.0%</div></div>
        </div>
        <div class="chart-row">
            <div class="chart-label">Gemma 2 (9B)</div>
            <div class="chart-bar-container"><div class="chart-bar" style="width: 89.6%; background: #85c1e9;">89.6%</div></div>
        </div>
        <div class="chart-row">
            <div class="chart-label">Qwen 2.5 (7B)</div>
            <div class="chart-bar-container"><div class="chart-bar" style="width: 87.2%; background: #aed6f1;">87.2%</div></div>
        </div>
    </div>
    <p class="chart-caption">Even 2B models transfer at 90%. The model learned SQL structure, not entity associations.</p>
</div>

<p>This is the strongest evidence for our core thesis. The model has learned <strong>SQL's compositional structure</strong>&mdash;the relational algebra of querying&mdash;not dataset-specific entity associations. SQL naturally separates structure from vocabulary: a <code>WHERE class_name = 'bedroom'</code> and a <code>WHERE class_name = 'aeroplane'</code> have identical structure. The fence-based loss masking ensures the model learns exactly this decomposition.</p>

<p>One concrete example that makes this vivid:</p>

<div class="query-box">
    <div class="q">"Rarest 4-object pairs in aeroplane?"</div>
    <div class="a">The model generates a character-identical 4-way self-join query over Pascal VOC body-part vocabulary (lear, torso, rhand) it has never seen. The critical ordering constraint <code>io1.object_id &lt; io2.object_id &lt; io3.object_id &lt; io4.object_id</code> that prevents duplicate combinations is reproduced exactly.</div>
</div>

<p>The model trained on "wall" and "bed" correctly handles "lear" and "torso" because it learned that entity names are <em>parameters</em>, not <em>patterns</em>.</p>

<hr>

<h2>What Doesn't Work (and Why That's Informative)</h2>

<p>We deliberately tested beyond the system's design envelope to understand its failure modes.</p>

<h3>Out-of-Distribution Phrasing</h3>

<p>When we rephrase trained query patterns in novel ways, accuracy drops to 45%. Some sub-patterns work well: nested questions (90%), negations (70%), zero-threshold queries (100%). Others fail: informal language (12%), double negation (0%).</p>

<div class="query-box">
    <div class="q">"yo how many street pics got water in them"</div>
    <div class="a">Returns count (13) instead of intended percentage. Correct entity extraction, wrong template selection.</div>
</div>

<p>The informal register ("yo", "pics", "got") triggers the wrong template. This isn't a fundamental limitation&mdash;adding informal phrasings to the synthetic data generator would fix it. But it reveals where the model's coverage ends: at the boundary of its training distribution's register diversity.</p>

<h3>Novel SQL Constructs</h3>

<p>Questions that require SQL syntax absent from training (window functions, CASE expressions, recursive CTEs) score near 0%. But the system handles this gracefully: it falls back to the closest known template and produces a <em>valid, executable</em> query 99.3% of the time. Wrong answers, not crashes.</p>

<p>Interestingly, some novel constructs <em>do</em> work through compositional generalization: chained filters (100%) and string pattern matching (100%) compose from simpler learned operations. The model can compose what it knows; it just can't extrapolate to fundamentally new SQL syntax.</p>

<hr>

<h2>The Deeper Lessons: What Generalizes Beyond This Paper</h2>

<p>Beyond the specific GLARE results, several principles here apply broadly. These are patterns I think other researchers should consider.</p>

<h3>1. Use LLMs for Structure, Not Content</h3>

<p>The most transferable lesson: when you need both flexibility and faithfulness, <strong>use the LLM to handle structure and let a deterministic system handle content</strong>.</p>

<p>GLARE uses the LLM to parse natural language into SQL (structural mapping). The actual answers come from database execution (deterministic content). This eliminates hallucination by construction&mdash;not by hoping the model is "careful enough," but by making hallucination architecturally impossible.</p>

<p>This pattern applies broadly:</p>
<ul>
    <li><strong>Medical diagnosis support</strong>: LLM parses symptoms into database queries over clinical guidelines, rather than generating diagnoses</li>
    <li><strong>Legal research</strong>: LLM translates legal questions into structured searches over case law databases</li>
    <li><strong>Financial analysis</strong>: LLM converts natural language questions into queries over verified financial data</li>
</ul>

<p>Anywhere you need trustworthy answers from flexible input, the "LLM as parser" pattern is worth considering.</p>

<h3>2. Synthetic Data + Structural Loss Masking = Domain-Agnostic Learning</h3>

<p>The combination of synthetic training data and fence-based loss masking produces a model that learns <em>task structure</em> rather than <em>domain vocabulary</em>. This is a general recipe:</p>

<ol>
    <li>Generate training examples synthetically by sampling parameters from a template</li>
    <li>Mask the loss to focus only on the structural output (SQL), not the parameter values</li>
    <li>At deployment, provide new parameter values (entity names) in the prompt</li>
</ol>

<p>The result: a model that transfers to new domains without retraining. Our 90.6% zero-shot cross-dataset transfer demonstrates this isn't theoretical&mdash;it works in practice.</p>

<p>This approach is viable for any domain where the <em>structure</em> of the task is fixed but the <em>vocabulary</em> changes: code generation for different APIs, form filling for different schemas, query generation for different databases.</p>

<h3>3. Template Taxonomies Beat End-to-End Generation</h3>

<p>We deliberately chose to organize queries into 24 templates rather than training the model to generate arbitrary SQL end-to-end. This might seem limiting, but it's actually enabling:</p>

<ul>
    <li><strong>Perfect coverage on supported types</strong>: 100% accuracy on 18/25 templates</li>
    <li><strong>Graceful degradation on unsupported types</strong>: Falls back to closest template, 99.3% execution rate</li>
    <li><strong>Easy extensibility</strong>: Adding a new question type requires defining one SQL template and regenerating data</li>
    <li><strong>Formal guarantees</strong>: Every generated SQL is a valid instance of a verified template</li>
</ul>

<p>The principle: <strong>constrained generation with broad template coverage beats unconstrained generation for safety-critical applications</strong>. When correctness matters more than novelty, templates win.</p>

<h3>4. Explanation is a Query Problem, Not a Generation Problem</h3>

<p>The broadest conceptual contribution: we should rethink what "explainability" means. The dominant paradigm is generation&mdash;compute an explanation, present it to the user. But this treats users as passive consumers.</p>

<p>GLARE reframes explanation as <strong>query answering</strong>. The explanation data already exists (computed once, stored in a database). The interface's job is to help users ask the right questions and retrieve relevant slices of that data.</p>

<p>This aligns with decades of research in social science showing that human explanation-seeking is iterative, question-driven, and contrastive. People don't want "here's how the model works." They want "does the model use feature X for class Y?" or "what distinguishes class A from class B?" The query paradigm matches this cognitive process.</p>

<h3>5. The Minimum Viable Model Is Smaller Than You Think</h3>

<p>Our 2B-parameter model performs identically to the 27B model on both in-distribution and cross-dataset transfer tasks. For deployment, this means:</p>

<ul>
    <li>GLARE can run on a single consumer GPU</li>
    <li>Inference is fast enough for interactive use</li>
    <li>The cost barrier to deploying an interactive explanation system is low</li>
</ul>

<p>More broadly, this suggests that for well-defined structural tasks (as opposed to open-ended generation), model capacity saturates quickly. If you can define your task precisely enough, you can serve it cheaply.</p>

<hr>

<h2>What We'd Do Differently (and What's Next)</h2>

<p>Being honest about limitations and future work:</p>

<p><strong>Multi-turn dialogue.</strong> GLARE currently handles single questions. But real explanation-seeking is iterative: "What objects appear in bedrooms?" &rarr; "Which of those also appear in living rooms?" &rarr; "Show me examples where they differ." Supporting conversational context and coreference resolution is the natural next step.</p>

<p><strong>Broader explanation types.</strong> We demonstrated with DNF-based explanations from one algorithm. The database schema and query templates should extend to other explanation formats: concept-based explanations, prototype networks, counterfactual explanations. The core idea&mdash;explanations as queryable databases&mdash;is format-agnostic.</p>

<p><strong>User studies.</strong> We evaluated SQL accuracy but haven't yet measured whether users actually <em>understand models better</em> with GLARE vs. traditional XAI dashboards. This is the missing piece: task-level human evaluation of explanation utility.</p>

<p><strong>Handling informal language.</strong> The 12% accuracy on informal register is a practical gap. Real users will say "yo how many pics got X" and expect it to work. Expanding the synthetic data generator's register coverage is straightforward engineering, not a research problem, but it matters for deployment.</p>

<hr>

<h2>Summary</h2>

<p>GLARE demonstrates that the bottleneck in explainable AI is not generating better explanations&mdash;it's making existing explanations accessible. By treating global explanations as relational databases and using LLMs as semantic parsers (not content generators), we build an interface that:</p>

<ul>
    <li>Achieves 95% query accuracy across diverse question types</li>
    <li>Transfers to new datasets at 90.6% without retraining</li>
    <li>Handles spelling errors, synonyms, and verbose input gracefully</li>
    <li>Eliminates hallucination by construction, not by hope</li>
    <li>Runs on 2B-parameter models with no performance loss</li>
</ul>

<p>The broader lesson: when you need trustworthy answers from flexible input, let the LLM handle structure and let a verified system handle content. This "LLM as parser" pattern sidesteps the hallucination problem entirely and applies far beyond explainability.</p>

<p>The even broader lesson: if your users are drowning in information, the solution isn't better information&mdash;it's a better question-and-answer interface. Explanations aren't documents. They're databases waiting for the right query.</p>

<hr>

<p><em>Paper and code available at the project website.</em></p>

</body>
</html>
